# -*- coding: utf-8 -*-
"""PhucThanhNguyen_nn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dedgphm5wx9_DjOtoWS-bU9q3s9-bCea

## Problem statement:

To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution which can evaluate images and alert the dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis.

## Importing Skin Cancer Data

## Importing all the important libraries
"""

# https://stackoverflow.com/questions/71000120/colab-0-unimplemented-dnn-library-is-not-found
# # Check libcudnn8 version
# !apt-cache policy libcudnn8

# # Install latest version
# !apt install --allow-change-held-packages libcudnn8=8.4.1.50-1+cuda11.6

# # Export env variables
# !export PATH=/usr/local/cuda-11.4/bin${PATH:+:${PATH}}
# !export LD_LIBRARY_PATH=/usr/local/cuda-11.4/lib64:$LD_LIBRARY_PATH
# !export LD_LIBRARY_PATH=/usr/local/cuda-11.4/include:$LD_LIBRARY_PATH
# !export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64

# # Install tensorflow
# !pip install tflite-model-maker==0.4.0
# !pip uninstall -y tensorflow && pip install -q tensorflow==2.9.1
# !pip install pycocotools==2.0.4
# !pip install opencv-python-headless==4.6.0.66

import pathlib
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os
import PIL
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
import glob

## If you are using the data by mounting the google drive, use the following :
from google.colab import drive
drive.mount('/content/gdrive')

##Ref:https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166

"""This assignment uses a dataset of about 2357 images of skin cancer types. The dataset contains 9 sub-directories in each train and test subdirectories. The 9 sub-directories contains the images of 9 skin cancer types respectively."""

# Defining the path for train and test images
## Todo: Update the paths of the train and test dataset
data_dir_train = pathlib.Path("/content/gdrive/MyDrive/LJMU/DeepLearning/Master_LJMU-IIITB_Course05_01/data/Train")
data_dir_test = pathlib.Path("/content/gdrive/MyDrive/LJMU/DeepLearning/Master_LJMU-IIITB_Course05_01/data/Test")

image_count_train = len(list(data_dir_train.glob('*/*.jpg')))
print(image_count_train)
image_count_test = len(list(data_dir_test.glob('*/*.jpg')))
print(image_count_test)

"""## Some analysis about no. classes"""

data_detail_pd = pd.DataFrame(columns=["Dir_Name","Total Image(Train)","Total Percentage(Train)","Total Image(Test)","Total Percentage(Test)"])
# train data in each folders
for dir_name in glob.glob(os.path.join(data_dir_train, "*")):
  total_image_in_folder = len(glob.glob(os.path.join(dir_name, "*.jpg")))
  df = {"Dir_Name":os.path.basename(dir_name),"Total Image(Train)":total_image_in_folder,"Total Percentage(Train)":round((total_image_in_folder/image_count_train)*100,2)}
  data_detail_pd = data_detail_pd.append(df,ignore_index=True)
data_detail_pd = data_detail_pd.set_index("Dir_Name")
# test data in each folders
for dir_name in glob.glob(os.path.join(data_dir_test, "*")):
  total_image_in_folder = len(glob.glob(os.path.join(dir_name, "*.jpg")))
  data_detail_pd.loc[os.path.basename(dir_name),"Total Image(Test)"]  = total_image_in_folder
  data_detail_pd.loc[os.path.basename(dir_name),"Total Percentage(Test)"]  = round((total_image_in_folder/image_count_test)*100,2)
# data_detail_pd = data_detail_pd.set_index("Dir_Name")
display(data_detail_pd.sort_values(by="Total Percentage(Train)",ascending=False))

"""Observation : Melanoma has 19.56% of data in train and 13.56% data in test data set.

---



Highest Sample of Data : pigmented benign keratosis (20.63%)

---


Lowest Sample of Data  : seborrheic keratosis (3.44% in train and 2.54% in test)
"""

plt.figure(figsize=(5,5))
plt.bar(data_detail_pd.index,data_detail_pd['Total Percentage(Train)'])
plt.xticks(rotation=90)
plt.title("Train Data Distribution")
plt.show()

plt.figure(figsize=(5,5))
plt.bar(data_detail_pd.index,data_detail_pd['Total Percentage(Test)'])
plt.xticks(rotation=90)
plt.title("Test Data Distribution")
plt.show()

"""Class imbalanced, distribution of data different in train and test

## Create a dataset

Define some parameters for the loader:
"""

batch_size = 32
img_height = 180
img_width = 180

"""Use 80% of the images for training, and 20% for validation."""

## Write your train dataset here
## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory
## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset
train_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir_train,
    labels='inferred',
    label_mode='int',
    class_names=None,
    color_mode='rgb',
    batch_size=batch_size,
    image_size=(img_height, img_width),
    shuffle=True,
    seed=123,
    validation_split=0.2,
    subset="training",
    interpolation='bicubic',
)

## Write your validation dataset here
## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory
## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset
val_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir_train,
    labels='inferred',
    label_mode='int',
    class_names=None,
    color_mode='rgb',
    batch_size=batch_size,
    image_size=(img_height, img_width),
    shuffle=True,
    seed=123,
    validation_split=0.2,
    subset="validation",
    interpolation='bicubic',
)

# List out all the classes of skin cancer and store them in a list. 
# You can find the class names in the class_names attribute on these datasets. 
# These correspond to the directory names in alphabetical order.
class_names = train_ds.class_names
print(class_names)

"""## Visualize the data"""

import matplotlib.pyplot as plt
import copy
### your code goes here, you can use training or validation data to visualize
plt.figure(figsize=(10, 10))

# get image each class
class_names_draw = [[] for i in range(len(class_names))]
count = 0
while count < len(class_names):
    for images, labels in train_ds.take(10):
        for i in range(batch_size):
            class_index = class_names.index(class_names[labels[i]])
            if class_names_draw[class_index] == []:
                class_names_draw[class_index] = images[i].numpy().astype("uint8")
                count +=1
    

for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(class_names_draw[i])
    plt.title(class_names[i])
    plt.axis("off")
plt.show()

"""The `image_batch` is a tensor of the shape `(32, 180, 180, 3)`. This is a batch of 32 images of shape `180x180x3` (the last dimension refers to color channels RGB). The `label_batch` is a tensor of the shape `(32,)`, these are corresponding labels to the 32 images.

`Dataset.cache()` keeps the images in memory after they're loaded off disk during the first epoch.

`Dataset.prefetch()` overlaps data preprocessing and model execution while training.
"""

AUTOTUNE = tf.data.experimental.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

"""## Create the model"""

input_shape = (180, 180, 3)
num_classes = 9

"""### Model 1: Only conv2d"""

## Your code goes here
from tensorflow.keras.layers import Input, Conv2D 
from tensorflow.keras.layers import MaxPool2D, Flatten, Dense, BatchNormalization, Dropout
from tensorflow.keras.layers.experimental.preprocessing import Rescaling
from tensorflow.keras import Model

model1 = Sequential([
         layers.Rescaling(1./255,input_shape=input_shape),
         layers.Conv2D(16,3,padding='same',activation="relu"),
         layers.MaxPool2D((2,2),strides=2),
         layers.Conv2D(32,3,padding='same',activation="relu"),
         layers.MaxPool2D((2,2),strides=2),
         layers.Conv2D(64,3,padding='same',activation="relu"),
         layers.MaxPool2D((2,2),strides=2),
         layers.Flatten(),
         layers.Dense(128,activation="relu"),
         layers.Dense(num_classes)
         ],
         name = 'Model_01')

"""#### Compile the model
Choose an appropirate optimiser and loss function for model training 
"""

### Todo, choose an appropirate optimiser and loss function
model1.compile(optimizer="adam",
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# View the summary of all layers
model1.summary()

"""#### Train the model"""

epochs = 20
history = model1.fit(train_ds, validation_data=val_ds, epochs=epochs)

"""#### Visualizing training results"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

"""#### Findings
1.   Training Accuracy   : Training Accuracy is high 
2.   Validation Accuracy : Validation accuracy is low compared to the Training Accuracy so , its not a good model.
3.   Training Loss       : Its decerasing
4.   Validation Loss     : Its decerasing until epoch 10 then increasing per epoch so not a good fit

### Model 2: With custom augmentation
"""

# Todo, after you have analysed the model fit history for presence of underfit or overfit, choose an appropriate data augumentation strategy. 
# Your code goes here
data_augument = tf.keras.Sequential([
    layers.experimental.preprocessing.RandomFlip(mode="horizontal_and_vertical",input_shape=input_shape),
    layers.experimental.preprocessing.RandomRotation(0.2, fill_mode='reflect'),
    layers.experimental.preprocessing.RandomZoom(height_factor=(0.2, 0.3), width_factor=(0.2, 0.3), fill_mode='reflect')
    ])

# Todo, visualize how your augmentation strategy works for one instance of training image.
# Your code goes here
plt.figure(figsize=(10, 10))

# get image each class
class_names_draw = [[] for i in range(len(class_names))]
count = 0
while count < len(class_names):
    for images, labels in train_ds.take(10):
        # augmentation images
        images = data_augument(images)
        for i in range(batch_size):
            class_index = class_names.index(class_names[labels[i]])
            if class_names_draw[class_index] == []:
                class_names_draw[class_index] = images[i].numpy().astype("uint8")
                count +=1
    

for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(class_names_draw[i])
    plt.title(class_names[i])
    plt.axis("off")
plt.show()
# _ = plt.title(get_label_name(label))

## You can use Dropout layer if there is an evidence of overfitting in your findings

## Your code goes here

model2 = Sequential([
    data_augument,
    layers.Rescaling(1./255,input_shape=(img_height,img_width,3)),
    layers.Conv2D(16,3,padding='same',activation="relu"),
    layers.MaxPool2D((2,2),strides=2),
    layers.Conv2D(32,3,padding='same',activation="relu"),
    layers.MaxPool2D((2,2),strides=2),
    layers.Conv2D(64,3,padding='same',activation="relu"),
    layers.MaxPool2D((2,2),strides=2),
    layers.Dropout(0.2), # droupout layer
    layers.Flatten(),
    layers.Dense(128,activation="relu"),
    layers.Dense(num_classes)
    ],
    name="Model_02")
model2.summary()

"""#### Compiling the model"""

## Your code goes here
### Todo, choose an appropirate optimiser and loss function
model2.compile(optimizer="adam",
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

"""#### Training the model"""

## Your code goes here, note: train your model for 20 epochs
epochs = 30
history = model2.fit(train_ds, validation_data=val_ds, epochs=epochs)

"""#### Visualizing the results"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

"""#### Findings:
1. The training accuracy go down (underfit), but the distance between train and validation go down to
2. Try to add more layers
3. The analysis before have show that class is imbalanced and ditribution of classes in train and test is different

### Model 3: Adding more layers
"""

model3 = Sequential([
    data_augument,
    layers.Rescaling(1./255,input_shape=(img_height,img_width,3)),
    layers.Conv2D(16,3,padding='same',activation="relu"),
    layers.MaxPool2D((2,2),strides=2),

    layers.Conv2D(32,3,padding='same',activation="relu"),
    layers.MaxPool2D((2,2),strides=2),
    layers.Dropout(0.25), # droupout layer

    layers.Conv2D(64,3,padding='same',activation="relu"),
    layers.MaxPool2D((2,2),strides=2),
    layers.Dropout(0.25), # droupout layer

    layers.Conv2D(128,3,padding='same',activation="relu"),
    layers.MaxPool2D((2,2),strides=2),
    layers.Dropout(0.25), # droupout layer

    layers.Flatten(),
    layers.Dense(128,activation="relu"),
    layers.Dropout(0.25), # droupout layer
    
    layers.Dense(num_classes)],
    name="Model_03"
    )
model3.summary()

"""#### Compile model"""

model3.compile(optimizer="adam",
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

"""#### Training the model"""

epochs = 50
history = model3.fit(train_ds, validation_data=val_ds, epochs=epochs)

"""#### Visualizing the results"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

"""#### Findings
Model has no Overfitting : as both train & validation accuracy move close to overlap (to epoch 30)

### Model 4: Adding BatchNorm
"""

model4 = Sequential([
    data_augument,
    layers.Rescaling(1./255,input_shape=(img_height,img_width,3)),
    layers.Conv2D(16,3,padding='same',activation="relu"),
    layers.MaxPool2D((2,2),strides=2),
    layers.BatchNormalization(),
    layers.Dropout(0.25), # droupout layer

    layers.Conv2D(32,3,padding='same',activation="relu"),
    layers.MaxPool2D((2,2),strides=2),
    layers.BatchNormalization(),
    layers.Dropout(0.25), # droupout layer

    layers.Conv2D(64,3,padding='same',activation="relu"),
    layers.MaxPool2D((2,2),strides=2),
    layers.BatchNormalization(),
    layers.Dropout(0.25), # droupout layer

    layers.Conv2D(128,3,padding='same',activation="relu"),
    layers.MaxPool2D((2,2),strides=2),
    layers.BatchNormalization(),
    layers.Dropout(0.25), # droupout layer

    layers.Flatten(),
    layers.Dense(128,activation="relu"),
    
    layers.Dense(num_classes)],
    name = "Model_04")

"""#### Compile model"""

model4.compile(optimizer="adam",
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

"""#### Training model"""

## Your code goes here, note: train your model for 20 epochs
epochs = 70
history = model4.fit(train_ds, validation_data=val_ds, epochs=epochs)

"""#### Visualizing the results"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

"""#### Findings
1. No Additional improvement, its due to very less number of data
2. Try improve number of data

### Model 5: BatchNorm with more image by Augmentor
"""

# install Augmentor
!pip install Augmentor

path_to_training_dataset= data_dir_train
import Augmentor
for i in class_names:
    p = Augmentor.Pipeline(os.path.join(path_to_training_dataset, i))
    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)
    p.sample(1000) ## We are adding 1000 samples per class to make sure that none of the classes are sparse.

"""Augmentor has stored the augmented images in the output sub-directory of each of the sub-directories of skin cancer types.. Lets take a look at total count of augmented images."""

image_count_train = len(list(data_dir_train.glob('*/output/*.jpg')))
print(image_count_train)

"""#### Lets see the distribution of augmented data after adding new images to the original training data."""

image_count_train = len(list(data_dir_train.glob('*/output/*.jpg'))) + len(list(data_dir_train.glob('*/*.jpg')))
print(image_count_train)

data_detail_pd = pd.DataFrame(columns=["Dir_Name","Total Image(Train)","Total Percentage(Train)","Total Image(Test)","Total Percentage(Test)"])
# train data in each folders
for dir_name in glob.glob(os.path.join(data_dir_train, "*")):
  total_image_in_folder = len(glob.glob(os.path.join(dir_name, "*.jpg"))) + len(glob.glob(os.path.join(dir_name, "output", "*")))
  df = {"Dir_Name":os.path.basename(dir_name),"Total Image(Train)":total_image_in_folder,"Total Percentage(Train)":round((total_image_in_folder/image_count_train)*100,2)}
  data_detail_pd = data_detail_pd.append(df,ignore_index=True)
data_detail_pd = data_detail_pd.set_index("Dir_Name")
# test data in each folders
for dir_name in glob.glob(os.path.join(data_dir_test, "*")):
  total_image_in_folder = len(glob.glob(os.path.join(dir_name, "*.jpg")))
  data_detail_pd.loc[os.path.basename(dir_name),"Total Image(Test)"]  = total_image_in_folder
  data_detail_pd.loc[os.path.basename(dir_name),"Total Percentage(Test)"]  = round((total_image_in_folder/image_count_test)*100,2)
# data_detail_pd = data_detail_pd.set_index("Dir_Name")
display(data_detail_pd.sort_values(by="Total Percentage(Train)",ascending=False))

plt.figure(figsize=(5,5))
plt.bar(data_detail_pd.index,data_detail_pd['Total Percentage(Train)'])
plt.xticks(rotation=90)
plt.title("Train Data Distribution")
plt.show()

"""So, now we have added 1000 images to all the classes to maintain some class balance. We can add more images as we want to improve training process."""

batch_size = 32
img_height = 180
img_width = 180

"""#### Create dataset"""

train_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir_train,
    labels='inferred',
    label_mode='int',
    class_names=None,
    color_mode='rgb',
    batch_size=batch_size,
    image_size=(img_height, img_width),
    shuffle=True,
    seed=123,
    validation_split=0.2,
    subset="training",
    interpolation='bicubic',
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir_train,
    labels='inferred',
    label_mode='int',
    class_names=None,
    color_mode='rgb',
    batch_size=batch_size,
    image_size=(img_height, img_width),
    shuffle=True,
    seed=123,
    validation_split=0.2,
    subset="validation",
    interpolation='bicubic',
)

"""#### Create final model"""

model5 = Sequential([
    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),

    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(pool_size = (2,2)),
    layers.BatchNormalization(),
    #layers.Dropout(0.25),

    layers.Conv2D(32, 3, padding='same', activation='relu'),
    #layers.MaxPooling2D(),
    #layers.BatchNormalization(),

    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(pool_size = (2,2)),
    layers.BatchNormalization(),

    layers.Conv2D(128, 3, padding='same', activation='relu'),

    layers.Conv2D(256, 3, padding='same', activation='relu'),
    #layers.BatchNormalization(),
    #layers.Dropout(0.2),

    layers.Flatten(),

    layers.Dropout(0.2),

    layers.Dense(256, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.2),

    layers.Dense(128, activation='relu'),
    layers.BatchNormalization(),

    layers.Dense(64, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.2),

    layers.Dense(32, activation='relu'),
    layers.BatchNormalization(),

    layers.Dense(num_classes)
    ],
    name="Model_05")
model5.summary()

"""#### Compile model"""

model5.compile(optimizer="adam",loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics = ['accuracy'])

"""#### Train model"""

epochs = 50
history = model5.fit(train_ds, validation_data=val_ds, epochs=epochs)

"""#### Visualize the model results"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

"""#### Findings
Model get better with 94% acc on train and 84% on validation

### Analyze your results here. Did you get rid of underfitting/overfitting? Did class rebalance help?
"""

test_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir_test,
    labels='inferred',
    label_mode='int',
    class_names=None,
    color_mode='rgb',
    batch_size=batch_size,
    image_size=(img_height, img_width),
    shuffle=True,
    seed=123,
    interpolation='bicubic',
)

loss , accuracy = model5.evaluate(test_ds)

print("Accuracy on test data ", accuracy)

"""### Conclusion
1. Accuracy on training data has increased by using Augmentor library

2. Model is still overfitting

3. The problem of overfitting can be solved by add more layer,neurons or adding dropout layers.

4. The Model can be further improved by tuning the hyperparameter
"""

